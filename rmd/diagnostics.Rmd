# Diagnostics and performance

## $\hat R$ and effective sample size

We begin by plotting the rank-normalised $\hat R$ values for each of the three models in accordance with @rhat, which provides an improved comparison of the between-chain and within-chain estimates for each model parameter. If the chains have not mixed well, then we expect this value of $\hat R$ to be larger than $1$. Contrarily, if we find that the $\hat R$ for all model parameters are below $1.05$, then we will conclude that the chains have mixed well and agree on the parameter estimates.

```{r}
bayesplot_grid(
  mcmc_rhat(rhat = rhat(weibull_model)),
  mcmc_rhat(rhat = rhat(weibull_cens)),
  mcmc_rhat(rhat = rhat(weibull_hier)),
  titles = c(
    "Pooled model",
    "Censored model",
    "Hierarchical model"
  )
)
```

We find that all our models have converged, which is good news for our parameter estimates. We now look at the effective sample size of our MCMC draws for each our three models. The effective sample size, $n_{\text{eff}}$, is an estimate of the number of independent draws from the posterior distribution are statistically important towards estimating a given parameter. We will plot the ratio of $n_{\text{eff}}$ to $N$, the total number of samples, and hope to find this ratio to be as large as possible, since a larger $n_{\text{eff}}$ is indicative of stability across simulated sequence and that the simulations suffice for practical purposes as is argued in @bda3.

```{r}
bayesplot_grid(
  mcmc_neff(neff_ratio(weibull_model)),
  mcmc_neff(neff_ratio(weibull_cens)),
  mcmc_neff(neff_ratio(weibull_hier)),
  titles = c("Pooled model", "Censored model", "Hierarchical model")
  )
```

We find that both the pooled model, model with censored data have very few parameters with a $n_{\text{eff}} / N$ ratio of below $0.1$, and thus we can assume that the simulations are stable and should suffice for practical purposes. However, the hierarchical model not considering censored data has some parameters with $n_{\text{eff}} \leq 0.1$, which is slightly concerning regarding the model's future performance.

## Divergence parameters

There is small number of divergent transitions, which is identified by `divergent__` being 1. Also, chains have a `treedepth__` of at most 10 and the default is 10. The maximum number is not exceeded, so the sampler does not hit its limit on the number of leapfrog steps, taken per iteration.

```{r}
# pooled model
wm_sampler_params = get_sampler_params(weibull_model, inc_warmup = TRUE)
summary(do.call(rbind, wm_sampler_params), digits = 2)
# censored model
wmc_sampler_params = get_sampler_params(weibull_cens, inc_warmup = TRUE)
summary(do.call(rbind, wmc_sampler_params), digits = 2)
# hierarchical model
hwm_sampler_params = get_sampler_params(weibull_hier, inc_warmup = TRUE)
summary(do.call(rbind, hwm_sampler_params), digits = 2)
```

## Intervals for posterior data

We will now plot the posterior distributions of the weights of our regressors in our GLMs and the shape parameter $\alpha$ for our three models, starting with our pooled GLM not considering censored data.

```{r}
weibull_posterior <- as.data.frame(weibull_model)
mcmc_areas(
  weibull_posterior,
  pars = colnames(weibull_posterior)[
    (colnames(weibull_posterior) %like% "beta") 
    | (colnames(weibull_posterior) %like% "alpha")
  ],
  prob = 0.8,
  # 80% intervals
  prob_outer = 0.99,
  # 99%
  point_est = "mean"
)
```

These posterior distributions of the $\beta$ parameters in this model are very clearly centered around the origin, and in some cases, may not be hugely informative to our variate. The $\alpha$ parameter seems to have well converged, and to a non-zero value which is a good sign. We move now to the model considering censored data.

```{r}
cens_weibull_posterior <- as.data.frame(weibull_cens)
mcmc_areas(
  cens_weibull_posterior,
  pars = colnames(cens_weibull_posterior)[
    (colnames(cens_weibull_posterior) %like% "beta") 
    | (colnames(cens_weibull_posterior) %like% "alpha")
  ],
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
)
```

Here we find more informative parameter estimates in terms of effect on the latent predictor than the uncensored data. 

For hierarchical model the results are shown down below.

```{r}
hier_posterior <- as.data.frame(weibull_hier)
mcmc_areas(
  hier_posterior,
  pars = colnames(hier_posterior)[
    (colnames(hier_posterior) %like% "beta") 
    | (colnames(hier_posterior) %like% "alpha")
  ],
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
  )
```

## Posterior predictive checks

```{r}
color_scheme_set("brightblue")
yrep_cens=extract(weibull_cens)$ypred
yrep_weib=extract(weibull_model)$ypred
yrep_hier=extract(weibull_hier)$ypred
bayesplot::bayesplot_grid(
  bayesplot::ppc_hist(y, yrep_weib[1:8, ]),
  bayesplot::ppc_hist(yobs, yrep_cens[1:8, ]),
  bayesplot::ppc_hist(y, yrep_hier[1:8, ]),
  titles = c("Pooled", "Censored", "Hierarchical")
)
```

For pooled model the posterior results are similar to original survival survival distribution. Here, the censored data is not inclulded, so all the data is mainly concentrated near particular peak.

For censored model the posterior draws are similar to the original distribution. However, for some replicates the peak of the distribution is a little bit lower than for original data.

For hierarchical model there are more broaden results, as here different institutional groups are taken into consideration. The predictive values in most cases follow the same distribution with, of course, some noise. 

## Model comparison using leave-one-out cross-validation and WAIC

Having briefly examined the posterior distributions of our models, we now investigate the PSIS-LOO and WAIC values for model comparison, in the aim of determining which of our models is "best" for our purposes. Note, that PSIS-LOO is more accurate than WAIC criteria in our case since PSIS provides useful diagnostics as well as effective sample size and Monte Carlo estimates, as is mentioned in @loo.

```{r}
# perform approximate loo and psis-loo
wm_log_lik <- extract_log_lik(weibull_model, merge_chains = FALSE)
# estimate the PSIS effective sample size
wm_r_eff <- relative_eff(exp(wm_log_lik), cores = parallel::detectCores())
# compute loo
wm_loo <- loo(wm_log_lik, r_eff = wm_r_eff, cores = parallel::detectCores())
# compute waic
wm_waic <- waic(wm_log_lik, cores = parallel::detectCores())
# repeat for censored data model
cwm_log_lik <- extract_log_lik(weibull_cens, merge_chains = FALSE)
cwm_r_eff <- relative_eff(exp(cwm_log_lik), cores = parallel::detectCores())
cwm_loo <- loo(cwm_log_lik, r_eff = cwm_r_eff, cores = parallel::detectCores())
cwm_waic <- waic(cwm_log_lik, cores = parallel::detectCores())
# repeat for hierarchical Weibull
hwm_log_lik <- extract_log_lik(weibull_hier, merge_chains = FALSE)
hwm_r_eff <- relative_eff(exp(hwm_log_lik), cores = parallel::detectCores())
hwm_loo <- loo(hwm_log_lik, r_eff = hwm_r_eff, cores = parallel::detectCores())
hwm_waic <- waic(hwm_log_lik, cores = parallel::detectCores())
```

```{r}
# plot pareto k diagnostics for the models
plot(wm_loo, label_points = TRUE)
plot(cwm_loo, label_points = TRUE)
plot(hwm_loo, label_points = TRUE)
```

```{r}
# compare loos
wm_loo
cwm_loo
hwm_loo
```

```{r}
# compare waics
wm_waic
cwm_waic
hwm_waic
```

Considering the Pareto $k$-values, for pooled model all values are good. For the censored model one value is larger than 1, all others are good Hierarchical model shows better performance in that case, as for it all $k$-values are lower than $0.7$ like the pooled model.

Comparing the ELPD, the best (largest) value is shown by hierarchical model, than comes the model with censored data, after that the pooled model. It is worth noting, however, that the censored model was trained on more data since it included censored patients. This means that in theory it should have learned more information than the other two, and would thus not be comparable. However, since we find a better ELPD for the hierarchical model even given the difference in training data, we are comfortable concluding that it is the best of the three.

Considering the `p_loo` values. For pooled the optimal number of parameters is ~5.5 and given that there 7 in the model, the result is pretty close. For censored ~13 params are shown and we actually have 15 parameters, which is again pretty close. For the hierarchical model, the `p_loo` shows 6.6 parameters.

## Predictive performance assessment

We will use ELPD with leave-one-out cross validation as is put forward in @loo to measure predictive performance on the basis that it is better than, for example, MSE for continuous variable prediction, as it evaluates the whole predictive distribution and not just the mean.

For the pooled model, $\text{ELPD} = -632$, for the pooled model censored using censored data, $\text{ELPD} = -527$, and for the hierarchical model, $\text{ELPD} = -486$. Knowing that all Pareto $\hat k$ values are reasonable, and that thus these values are reliable, we choose the hierarchical model as the best fitting model for our data.

## Prior sensitivity analysis

The sensitivity of the posterior distribution of our sampled parameters to the proposed prior distribution was checked for models.

The dataset is not very small and posterior inferences based on large numbers of MCMC iterations tend to be not particularly sensitive to the prior distribution. The parameters of half-Cauchy were changed a bit, as well as the Normal distribution for betas. The combinations tested were: $\text{Cauchy}(0,8)$ and $\text{Normal}(0,10)$, $\text{Cauchy}(0,2)$ and $\text{Normal}(0,10)$, $\text{Cauchy}(0,5)$ and $\text{Normal}(0,20)$, $\text{Cauchy}(0,8)$ and $\text{Normal}(0,20)$. The results in most cases remained the same.

In order to test the motivation set out in previous sections and in @taumain, the variance hyperparameter for the hierarchical model was changed to a Half-Cauchy (the suggested priors for fewer hierarchical groups), and in this case convergence was more difficult. As such, the rigidity and relative strength of the $\text{Gamma}(1, 1)$ helps convergence in this case with more groups. It is also worth noting, however, that changing the prior over the global $\alpha$ parameter from Half-Cauchy to Gamma has a large impact on the final result. We can draw from this information that the relative flexibility afforded by the weakly-informative Half-Cauchy prior allows the data to express itself more compared to the stronger and less flexible Gamma prior.


