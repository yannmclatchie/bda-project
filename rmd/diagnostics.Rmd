# Diagnostics and performance

## $\hat R$, effective sample size, and divergences

We begin by plotting the rank-normalised $\hat R$ values for each of the three models in accordance with @rhat, which provides an improved comparison of the between-chain and within-chain estimates for each model parameter. If the chains have not mixed well, then we expect this value of $\hat R$ to be larger than $1$. Contrarily, if we find that the $\hat R$ for all model parameters are below $1.05$, then we will conclude that the chains have mixed well and agree on the parameter estimates.

```{r}
bayesplot_grid(
  mcmc_rhat(rhat = rhat(weibull_model)),
  mcmc_rhat(rhat = rhat(weibull_hier)),
  mcmc_rhat(rhat = rhat(weibull_cens)),
  titles = c("Pooled model", "Hierarchical model", "Censored model")
  )
```

We find that all our models have converged, which is good news for our parameter estimates. We now look at the effective sample size of our MCMC draws for each our three models. The effective sample size, $n_{\text{eff}}$, is an estimate of the number of independent draws from the posterior distribution are statistically important towards estimating a given parameter. We will plot the ratio of $n_{\text{eff}}$ to $N$, the total number of samples, and hope to find this ratio to be as large as possible, since a larger $n_{\text{eff}}$ is indicative of stability across simulated sequence and that the simulations suffice for practical purposes as is argued in @bda3.

```{r}
bayesplot_grid(
  mcmc_neff(neff_ratio(weibull_model)),
  mcmc_neff(neff_ratio(weibull_hier)),
  mcmc_neff(neff_ratio(weibull_cens)),
  titles = c("Pooled model", "Hierarchical model", "Censored model")
  )
```

We find that both the pooled models (not considered censored, and considered censored data respectively) have very few parameters with a $n_{\text{eff}} / N$ ratio of below $0.1$, and thus we can assume that the simulations are stable and should suffice for practical purposes. However, the hierarchical model not considering censored data have many such parameters with $n_{\text{eff}}$ small, which leads us to be doubtful of the models future performance.

## Divergence parameters

Censored model:
There is small number of divergent transitions, which is identified by divergent__ being 1.
Also, chains have a treedepth__ of at most 10 and the default is 10. The maximum number is not exceeded,
so the sampler does not hit its limit on the number of leapfrog steps, taken per iteration.
```{r}
wmc_sampler_params = get_sampler_params(weibull_cens, inc_warmup = TRUE)
summary(do.call(rbind, wmc_sampler_params), digits = 2)
```

## Intervals for posterior data

We will now plot the posterior distributions of the weights of our regressors in our GLMs and the shape parameter $\alpha$ for our three models, starting with our pooled GLM not considering censored data.

```{r}
regression_pars <- c(
  "beta[1]",
  "beta[2]",
  "beta[3]",
  "beta[4]",
  "beta[5]",
  "beta[6]",
  "beta[7]",
  "alpha"
)
weibull_posterior <- as.array(weibull_model)
mcmc_areas(
  weibull_posterior,
  pars = regression_pars,
  prob = 0.8,
  # 80% intervals
  prob_outer = 0.99,
  # 99%
  point_est = "mean"
)
```

These posterior distributions of the $\beta$ parameters in this model are very clearly centered around the origin, and in some cases, may not be hugely informative to our variate. The $\alpha$ parameter seems to have well converged, and to a non-zero value which is a good sign. We move now to the pooled model considering censored data.

```{r}
cens_weibull_posterior <- as.array(weibull_cens)
mcmc_areas(
  cens_weibull_posterior,
  pars = regression_pars,
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
)
```

Here we find more informative parameter estimates in terms of effect on the latent predictor than the uncensored data. Our hierarchical model, built in BRMS, does not log the regressor weights for the latent linear predictor, and instead operates in terms of correlations between variables given their group. Instead, we can visualise the population-level intercept of our model, `b_Intercept`, and the family `shape` parameter, seen below.

```{r}
hier_posterior <- as.array(weibull_hier)
mcmc_areas(
  hier_posterior,
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
  )
```

We find that the intercept is significantly non-zero, which suggests that
$$
\boldsymbol X \beta \ne 0
$$
for our data and latent regressors. This BRMS model uses a log link function, shown in the code
```
for (n in 1:N) {
  // apply the inverse link function
  mu[n] = exp(mu[n]) / tgamma(1 + 1 / shape);
}
target += weibull_lpdf(Y | shape, mu);
```
so that
$$
\sigma = \exp(\boldsymbol X \beta).
$$
Equally, we find a shape parameter $\alpha$, also significantly non-zero, but existing just below the values of $2$.

## Posterior predictive checks

For censored model the posterior draws are similar to the original distribution, however, there are two main differences.
In original data the peak of the distribution is larger, also, it is a little bit more narrow, for model draws sometimes wider distribution is received.
```{r}
color_scheme_set("brightblue")
yrep_cens=extract(weibull_cens)$ypred
bayesplot::ppc_hist(yobs, yrep_cens[1:8, ])
```

## Model comparison using leave-one-out cross-validation and WAIC

Having briefly examined the posterior distributions of our models, we now investigate the PSIS-LOO and WAIC values for model comparison, in the aim of determining which of our models is "best" for our purposes. Note, that PSIS-LOO is more accurate than WAIC criteria as it has better diagnostics.

```{r}
# perform approximate loo and psis-loo
wm_log_lik <- extract_log_lik(weibull_model, merge_chains = FALSE)
# estimate the PSIS effective sample size
wm_r_eff <- relative_eff(exp(wm_log_lik), cores = parallel::detectCores())
# compute loo
wm_loo <- loo(wm_log_lik, r_eff = wm_r_eff, cores = parallel::detectCores())
# compute waic
wm_waic <- waic(wm_log_lik, cores = parallel::detectCores())
# repeat for censored data model
cwm_log_lik <- extract_log_lik(weibull_cens, merge_chains = FALSE)
cwm_r_eff <- relative_eff(exp(cwm_log_lik), cores = parallel::detectCores())
cwm_loo <- loo(cwm_log_lik, r_eff = cwm_r_eff, cores = parallel::detectCores())
cwm_waic <- waic(cwm_log_lik, cores = parallel::detectCores())
# repeat for hierarchical Weibull
hwm_log_lik <- extract_log_lik(weibull_hier, merge_chains = FALSE)
hwm_r_eff <- relative_eff(exp(hwm_log_lik), cores = parallel::detectCores())
hwm_loo <- loo(hwm_log_lik, r_eff = hwm_r_eff, cores = parallel::detectCores())
hwm_waic <- waic(hwm_log_lik, cores = parallel::detectCores())
```

```{r}
# plot pareto k diagnostics for the models
plot(wm_loo, label_points = TRUE)
plot(cwm_loo, label_points = TRUE)
plot(hwm_loo, label_points = TRUE)
```

```{r}
# comparison of uncensored models with loo
loo_compare(x = list(wm_loo, hwm_loo))
# output loos
wm_loo
cwm_loo
hwm_loo
```

```{r}
# comparison of uncensored models with waic
loo_compare(x = list(wm_waic, hwm_waic))
# output waics
wm_waic
cwm_waic
hwm_waic
```

The best model is.....
After her comes the....

## Predictive performance assessment

Elpd_loo is better than, for example, MSE for continuous variable prediction, as it evaluates the whole predictive distribution and not just the mean. 


## Prior sensitivity analysis
The sensitivity of posterior inference about Î¸ to the proposed prior distribution has been checked for models.
The dataset in that work is not small and posterior inferences based on a large sample are not particularly sensitive to the prior distribution.

