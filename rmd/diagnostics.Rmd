# Diagnostics and performance

## $\hat R$ and effective sample size

We begin by plotting the rank-normalised $\hat R$ values for each of the three models in accordance with @rhat, which provides an improved comparison of the between-chain and within-chain estimates for each model parameter. If the chains have not mixed well, then we expect this value of $\hat R$ to be significantly larger than $1$. Contrarily, if we find that the $\hat R$ for all model parameters are below $1.05$, then we will conclude that the chains have mixed well and agree on the parameter estimates. Below, we plot the $\hat R$ values of each parameter for all three models.

```{r}
bayesplot_grid(
  mcmc_rhat(rhat = rhat(weibull_model)),
  mcmc_rhat(rhat = rhat(weibull_cens)),
  mcmc_rhat(rhat = rhat(weibull_hier)),
  titles = c(
    "Pooled model",
    "Censored model",
    "Hierarchical model"
  )
)
```

We find that all our models have converged and that the chains agree on all parameters, which is good news for our parameter estimates. We now look at the effective sample size of our MCMC draws for each our three models. The effective sample size, $n_{\text{eff}}$, is an estimate of the number of independent draws from the posterior distribution the are statistically important towards estimating a given parameter, as is defined in @bda3. We will plot the ratio of $n_{\text{eff}}$ to $N$, the total number of samples, and hope to find this ratio to be as large as possible, since a larger $n_{\text{eff}}$ is indicative of stability across a simulated sequence, and @bda3 argue that this indicates the simulations suffice for practical purposes.

```{r}
bayesplot_grid(
  mcmc_neff(neff_ratio(weibull_model)),
  mcmc_neff(neff_ratio(weibull_cens)),
  mcmc_neff(neff_ratio(weibull_hier)),
  titles = c("Pooled model", "Censored model", "Hierarchical model")
  )
```

We find that both the pooled models have no parameters with a $n_{\text{eff}} / N$ ratio of below $0.1$, which we consider to be the critical threshold as is given in @bda3, and thus we can assume that the simulations are stable and should suffice for practical purposes. However, the hierarchical model not considering censored data has some parameters with $n_{\text{eff}} / N \leq 0.1$, which is slightly concerning regarding the model's future performance.

## Divergence parameters

As is explained in the Stan documentation, for target distributions whose features are difficult to resolve, the MCMC may miss some samples relating to these features and thus returun a biased estimate, which manifests itself as a divergence. There are no divergent transitions in any of our three model, which is identified by `divergent__` being equal to $0$. 

Also, chains have a `treedepth__` of at most 10 which is also the default, and a much lower average. Since the maximum number is not exceeded, the sampler does not hit its limit on the number of leapfrog steps taken per iteration. This does not impact the validity of our estimates, but if we had exceeded the maximum `treedepth`, then our sampling may have been inefficient.

```{r}
# pooled model
wm_sampler_params = get_sampler_params(weibull_model, inc_warmup = TRUE)
summary(do.call(rbind, wm_sampler_params), digits = 2)
# censored model
wmc_sampler_params = get_sampler_params(weibull_cens, inc_warmup = TRUE)
summary(do.call(rbind, wmc_sampler_params), digits = 2)
# hierarchical model
hwm_sampler_params = get_sampler_params(weibull_hier, inc_warmup = TRUE)
summary(do.call(rbind, hwm_sampler_params), digits = 2)
```

We can plot the log-posterior and acceptance NUTS acceptance statistics given divergent samples for all three of our models below. 

```{r}
# plot divergence statistics for the pooled model
bayesplot::mcmc_nuts_divergence(nuts_params(weibull_model), log_posterior(weibull_model))
# plot divergence statistics for the censored model
bayesplot::mcmc_nuts_divergence(nuts_params(weibull_cens), log_posterior(weibull_cens))
# plot divergence statistics for the hierarhical model
bayesplot::mcmc_nuts_divergence(nuts_params(weibull_hier), log_posterior(weibull_hier))
```

We find that there are no divergences following the warmup, which means our posterior estimates in our target distribution less likely to be biased.

## Intervals for posterior data

We will now plot the posterior distributions of the weights of our regressors in our GLMs and the shape parameter $\alpha$ for our three models, starting with our pooled GLM not considering censored data.

```{r}
weibull_posterior <- as.data.frame(weibull_model)
mcmc_areas(
  weibull_posterior,
  pars = colnames(weibull_posterior)[
    (colnames(weibull_posterior) %like% "beta")
    | (colnames(weibull_posterior) %like% "alpha")
  ],
  prob = 0.8,
  # 80% intervals
  prob_outer = 0.99,
  # 99%
  point_est = "mean"
)
```

These posterior distributions of the $\beta$ parameters in this model are very clearly centered around the origin, and in some cases, may not be hugely informative to our variate. The $\alpha$ parameter seems to have well converged, and to a non-zero value which is a good sign. We move now to the model considering censored data.

```{r}
cens_weibull_posterior <- as.data.frame(weibull_cens)
mcmc_areas(
  cens_weibull_posterior,
  pars = colnames(cens_weibull_posterior)[
    (colnames(cens_weibull_posterior) %like% "beta")
    | (colnames(cens_weibull_posterior) %like% "alpha")
  ],
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
)
```

Here we find more informative parameter estimates in terms of effect on the latent predictor than the uncensored data. For hierarchical model the results are shown down below.

```{r}
hier_posterior <- as.data.frame(weibull_hier)
mcmc_areas(
  hier_posterior,
  pars = colnames(hier_posterior)[
    (colnames(hier_posterior) %like% "beta")
    | (colnames(hier_posterior) %like% "alpha")
  ],
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
  )
```

Once more we find a lot of regressor parameters including zero in their 80% credible interval, as well as several more significant parameters. We also find that the signficant parmeters differ slightly between institutions, suggesting that the hazard does change as a function of which institution a patient has been admitted to. Causality, of course, can not be concluded however.

## Posterior predictive checks

```{r}
color_scheme_set("brightblue")
yrep_cens=extract(weibull_cens)$ypred
yrep_weib=extract(weibull_model)$ypred
yrep_hier=extract(weibull_hier)$ypred
bayesplot::bayesplot_grid(
  bayesplot::ppc_hist(y, yrep_weib[1:8, ]),
  bayesplot::ppc_hist(yobs, yrep_cens[1:8, ]),
  bayesplot::ppc_hist(y, yrep_hier[1:8, ]),
  titles = c("Pooled", "Censored", "Hierarchical")
)
```

For pooled model the posterior results are similar to original survival survival distribution. Here, the censored data is not inclulded, so all the data is mainly concentrated near particular peak. For censored model the posterior draws are similar to the original distribution. However, for some replicates the peak of the distribution is a little bit lower than for original data. For hierarchical model there are more broaden results, as here different institutional groups are taken into consideration. The predictive values in most cases follow the same distribution with, of course, some noise.

All in all, the posterior predictive distributions from our three models seem to match the original data, which suggests that the models are reasonable in their desgin and have produced good predictions.

## Model comparison using leave-one-out cross-validation and WAIC

Having briefly examined the posterior distributions of our models, we now investigate the PSIS-LOO and WAIC values for model comparison, in the aim of determining which of our models is "best" for our purposes. This will be the model with the highest ELPD value. Note, that PSIS-LOO is more accurate than WAIC criteria in our case since PSIS provides useful diagnostics as well as effective sample size and Monte Carlo estimates, as is mentioned in @loo.

The PSIS-LOO and WAIC values are computed in accordance with the vignettes from @loo below.

```{r}
# perform approximate loo and psis-loo
wm_log_lik <- extract_log_lik(weibull_model, merge_chains = FALSE)
# estimate the PSIS effective sample size
wm_r_eff <- relative_eff(exp(wm_log_lik), cores = parallel::detectCores())
# compute loo
wm_loo <- loo(wm_log_lik, r_eff = wm_r_eff, cores = parallel::detectCores())
# compute waic
wm_waic <- waic(wm_log_lik, cores = parallel::detectCores())
# repeat for censored data model
cwm_log_lik <- extract_log_lik(weibull_cens, merge_chains = FALSE)
cwm_r_eff <- relative_eff(exp(cwm_log_lik), cores = parallel::detectCores())
cwm_loo <- loo(cwm_log_lik, r_eff = cwm_r_eff, cores = parallel::detectCores())
cwm_waic <- waic(cwm_log_lik, cores = parallel::detectCores())
# repeat for hierarchical Weibull
hwm_log_lik <- extract_log_lik(weibull_hier, merge_chains = FALSE)
hwm_r_eff <- relative_eff(exp(hwm_log_lik), cores = parallel::detectCores())
hwm_loo <- loo(hwm_log_lik, r_eff = hwm_r_eff, cores = parallel::detectCores())
hwm_waic <- waic(hwm_log_lik, cores = parallel::detectCores())
```

We now plot the Pareto $k$ values for all data points from our three models below.

```{r}
# plot pareto k diagnostics for the models
plot(wm_loo, label_points = TRUE)
plot(cwm_loo, label_points = TRUE)
plot(hwm_loo, label_points = TRUE)
```


We find once more than no data points have $\hat k > 0.7$ in either the pooled model without censored data or the hierarhical model, and thus we are able to conclude that the PSIS-LOO estimates are reliable. Some data points have $\hat k > 0.5$, which harms our confidence in the estimates slightly, but is still considered "ok". There is one data point with $\hat k > 0.7$ in the pooled model considering censored data, suggesting that there is potential bias in our model, and it might be over-estimating the predictive accuracy of the model. However, one high value is not enough for us to completely discard the model.

Having established that our Pareto $\hat k$ values of sufficiently good to consider the PSIS-LOO estimates to be reliable, we move on to display them.

```{r}
# compare loos
wm_loo
cwm_loo
hwm_loo
```

Comparing the ELPD, the best model (with the highest ELPD) is the hierarchical model, followed by the pooled model with censored data, and finally the pooled model without censored data. It is worth noting, however, that the censored model was trained on more data since it included censored patients. This means that in theory it should have learned more information than the other two, and is not necessarily comparable. However, since we find a better ELPD for the hierarchical model even given the difference in training data, we are comfortable concluding that it is the best of the three.

Considering the `p_loo` values. For pooled the effictive number of parameters is ~5.5 and given that there 7 in the model, this suggests that our parameters are mostly significant. For the censored pooled model ~13 effective parameters are shown and we actually have 15 parameters, which is again demostrative of significant parameters. For the hierarchical model, the `p_loo` shows 6.6 parameters which is much fewer than were trained, and so we understand that most of them are not significant. In this sense, the censored model on pooled data has found the most effective parameters.

We show also the WAIC values below, and achieve the same conclusion although understanding that our LOO values are more accurate.

```{r}
# compare waics
wm_waic
cwm_waic
hwm_waic
```

## Predictive performance assessment

We will use ELPD with leave-one-out cross validation as is put forward in @loo to measure predictive performance on the basis that it is better than, for example, MSE for continuous variable prediction, as it evaluates the whole predictive distribution and not just the mean. This is also the chosen metric is other similar literature.

For the pooled model not using censored data, $\text{ELPD} = -632$, for the pooled model using censored data, $\text{ELPD} = -527$, and for the hierarchical model, $\text{ELPD} = -486$. Knowing that all Pareto $\hat k$ values are reasonable, and that thus these values are reliable, we choose the hierarchical model as the best fitting model for our data.

## Prior sensitivity analysis

The sensitivity of the posterior distribution of our sampled parameters to the proposed prior distribution was checked for all three models with different distribution parameters, and in some cases different distributions.

The dataset is not very small and posterior inferences are based on large numbers of MCMC iterations which do not tend to be particularly sensitive to the prior distribution. The parameters of half-Cauchy were changed a bit, as well as the Normal distribution for betas. The combinations tested were: $\text{Cauchy}(0,8)$ and $\text{Normal}(0,10)$, $\text{Cauchy}(0,2)$ and $\text{Normal}(0,10)$, $\text{Cauchy}(0,5)$ and $\text{Normal}(0,20)$, $\text{Cauchy}(0,8)$ and $\text{Normal}(0,20)$. The results in most cases remained the same.

In order to test the motivation set out in previous sections and in @taumain regarding the prior over the hyperparameter $\sigma_\beta$ in the hierarchical model, its hyperprior was changed to a Half-Cauchy (the suggested priors for fewer hierarchical groups in), and in this case convergence was more difficult. As such, the rigidity and relative strength of the $\text{Gamma}(1, 1)$ helps convergence in this case with more groups and fewer data points for each group. It is also worth noting, however, that changing the prior over the global $\alpha$ parameter from Half-Cauchy to Gamma has a large impact on the final result, and ultimately did not result in convergence. We can draw from this information that the relative flexibility afforded by the weakly-informative Half-Cauchy prior allows the data to express itself more compared to the stronger and less flexible Gamma prior.

