# Diagnostics and performance

## $\hat R$, effective sample size, and divergences

We begin by plotting the rank-normalised $\hat R$ values for each of the three models in accordance with @rhat, which provides an improved comparison of the between-chain and within-chain estimates for each model parameter. If the chains have not mixed well, then we expect this value of $\hat R$ to be larger than $1$. Contrarily, if we find that the $\hat R$ for all model parameters are below $1.05$, then we will conclude that the chains have mixed well and agree on the parameter estimates.

```{r}
bayesplot_grid(
  mcmc_rhat(rhat = rhat(weibull_model)),
  mcmc_rhat(rhat = rhat(weibull_hier)),
  mcmc_rhat(rhat = rhat(weibull_cens)),
  titles = c("Pooled model", "Hierarchical model", "Censored model")
  )
```

We find that all our models have converged, which is good news for our parameter estimates. We now look at the effective sample size of our MCMC draws for each our three models. The effective sample size, $n_{\text{eff}}$, is an estimate of the number of independent draws from the posterior distribution are statistically important towards estimating a given parameter. We will plot the ratio of $n_{\text{eff}}$ to $N$, the total number of samples, and hope to find this ratio to be as large as possible, since a larger $n_{\text{eff}}$ is indicative of stability across simulated sequence and that the simulations suffice for practical purposes as is argued in @bda3.

```{r}
bayesplot_grid(
  mcmc_neff(neff_ratio(weibull_model)),
  mcmc_neff(neff_ratio(weibull_hier)),
  mcmc_neff(neff_ratio(weibull_cens)),
  titles = c("Pooled model", "Hierarchical model", "Censored model")
  )
```

We find that both the pooled model, model with censored data have very few parameters with a $n_{\text{eff}} / N$ ratio of below $0.1$, and thus we can assume that the simulations are stable and should suffice for practical purposes. However, the hierarchical model not considering censored data have a little more such parameters with $n_{\text{eff}}$ small, which leads us to be doubtful of the models future performance.

## Divergence parameters

There is small number of divergent transitions, which is identified by divergent__ being 1.
Also, chains have a treedepth__ of at most 10 and the default is 10. The maximum number is not exceeded,
so the sampler does not hit its limit on the number of leapfrog steps, taken per iteration.

```{r}
# pooled model
wm_sampler_params = get_sampler_params(weibull_model, inc_warmup = TRUE)
summary(do.call(rbind, wm_sampler_params), digits = 2)
# censored model
wmc_sampler_params = get_sampler_params(weibull_cens, inc_warmup = TRUE)
summary(do.call(rbind, wmc_sampler_params), digits = 2)
# hierarchical model
hwm_sampler_params = get_sampler_params(weibull_hier, inc_warmup = TRUE)
summary(do.call(rbind, hwm_sampler_params), digits = 2)
```

## Intervals for posterior data

We will now plot the posterior distributions of the weights of our regressors in our GLMs and the shape parameter $\alpha$ for our three models, starting with our pooled GLM not considering censored data.

```{r}
regression_pars <- c(
  "beta[1]",
  "beta[2]",
  "beta[3]",
  "beta[4]",
  "beta[5]",
  "beta[6]",
  "beta[7]",
  "alpha"
)
weibull_posterior <- as.data.frame(weibull_model)
mcmc_areas(
  weibull_posterior,
  pars = regression_pars,
  prob = 0.8,
  # 80% intervals
  prob_outer = 0.99,
  # 99%
  point_est = "mean"
)
```

These posterior distributions of the $\beta$ parameters in this model are very clearly centered around the origin, and in some cases, may not be hugely informative to our variate. The $\alpha$ parameter seems to have well converged, and to a non-zero value which is a good sign. We move now to the model considering censored data.

```{r}
cens_weibull_posterior <- as.data.frame(weibull_cens)
mcmc_areas(
  cens_weibull_posterior,
  pars = regression_pars,
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
)
```

Here we find more informative parameter estimates in terms of effect on the latent predictor than the uncensored data. 

For hierarchical model the results are shown down below.

```{r}
hier_posterior <- as.data.frame(weibull_hier)
mcmc_areas(
  hier_posterior,
  pars = colnames(hier_posterior)[
    (colnames(hier_posterior) %like% "beta") 
    | (colnames(hier_posterior) %like% "alpha")
  ],
  prob = 0.8,
  prob_outer = 0.99,
  point_est = "mean"
  )
```

## Posterior predictive checks

```{r}
color_scheme_set("brightblue")
yrep_cens=extract(weibull_cens)$ypred
yrep_weib=extract(weibull_model)$ypred
yrep_hier=extract(weibull_hier)$ypred
bayesplot::bayesplot_grid(
  bayesplot::ppc_hist(y, yrep_weib[1:8, ]),
  bayesplot::ppc_hist(yobs, yrep_cens[1:8, ]),
  bayesplot::ppc_hist(y, yrep_hier[1:8, ]),
  titles = c("Pooled", "Censored", "Hierarchical")
)
```

For pooled model the posterior results are similar to original survival survival distribution. Here, the censored data is not inclulded, so all the data is mainly concentrated near particular peak.

For censored model the posterior draws are similar to the original distribution. However, for some replicates the peak of the distribution is a little bit lower than for original data.

For hierarchical model there are more broaden results, as here different institutional groups are taken into consideration. The predictive values in most cases follow the same distribution with, of course, some noise. 

## Model comparison using leave-one-out cross-validation and WAIC

Having briefly examined the posterior distributions of our models, we now investigate the PSIS-LOO and WAIC values for model comparison, in the aim of determining which of our models is "best" for our purposes. Note, that PSIS-LOO is more accurate than WAIC criteria as it has better diagnostics.

```{r}
# perform approximate loo and psis-loo
wm_log_lik <- extract_log_lik(weibull_model, merge_chains = FALSE)
# estimate the PSIS effective sample size
wm_r_eff <- relative_eff(exp(wm_log_lik), cores = parallel::detectCores())
# compute loo
wm_loo <- loo(wm_log_lik, r_eff = wm_r_eff, cores = parallel::detectCores())
# compute waic
wm_waic <- waic(wm_log_lik, cores = parallel::detectCores())
# repeat for censored data model
cwm_log_lik <- extract_log_lik(weibull_cens, merge_chains = FALSE)
cwm_r_eff <- relative_eff(exp(cwm_log_lik), cores = parallel::detectCores())
cwm_loo <- loo(cwm_log_lik, r_eff = cwm_r_eff, cores = parallel::detectCores())
cwm_waic <- waic(cwm_log_lik, cores = parallel::detectCores())
# repeat for hierarchical Weibull
hwm_log_lik <- extract_log_lik(weibull_hier, merge_chains = FALSE)
hwm_r_eff <- relative_eff(exp(hwm_log_lik), cores = parallel::detectCores())
hwm_loo <- loo(hwm_log_lik, r_eff = hwm_r_eff, cores = parallel::detectCores())
hwm_waic <- waic(hwm_log_lik, cores = parallel::detectCores())
```

```{r}
# plot pareto k diagnostics for the models
plot(wm_loo, label_points = TRUE)
plot(cwm_loo, label_points = TRUE)
plot(hwm_loo, label_points = TRUE)
```

```{r}
# compare loos
wm_loo
cwm_loo
hwm_loo
```

```{r}
# compare waics
wm_waic
cwm_waic
hwm_waic
```

Considering the k-values, for pooled model there is one bad value (larger than 0.7), others are good. For censored model one value is larger than 1, others are correct. Hierarchical model shows better performance in that case, as for it all k-values are lower than 0.7.

Comparing the elpd_loo, the best (largest) value is shown by hierarchical model, than comes the model with censored data, after that - simple pooled.

Considering the p_loo values. For pooled the optimal number of parameters is ~5.5 (and, actually, there 7 in the model, the result is pretty close).
For censored ~13 params are shown (there actually we have 15 parameters, agan the results are pretty close). For hierarchical model, the p_loo shows 6.6 parameters

## Predictive performance assessment

Elpd_loo is better than, for example, MSE for continuous variable prediction, as it evaluates the whole predictive distribution and not just the mean.
For pooled elpd_loo = -632, for censored= - 527, for hierarchical = -486.

## Prior sensitivity analysis
The sensitivity of posterior inference about $\theta$ to the proposed prior distribution has been checked for models.
The dataset in that work is not really small and posterior inferences based on a large sample are not particularly sensitive to the 
prior distribution. The parameters of half-cauchy were changed a bit, as well as the Normal distribution for betas.
The combinations tested were: cauchy(0,8) and Normal(0,10), cauchy(0,2) and Normal(0,10), cauchy(0,5) and Normal(0,20), cauchy(0,8) and Normal(0,20).
The results in most cases remained the same.

